{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import allel\n",
    "import zarr\n",
    "import numcodecs\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the command line arguements\n",
    "# - validate the arguements \n",
    "# - throw errors if requiremabents are missing\n",
    "# - validate the filter strings\n",
    "\n",
    "#pi_dxy --pi --dxy\\ # must include at least 1 of pi and/dxy \n",
    "#--vcf allsites.vcf.gz \\ # required\n",
    "#--zarr path/to/zarr \\  # default to the vcf folder\n",
    "#--populations popfile.txt \\ # only required for dxy\n",
    "#--window_size 10000 \\ # not required, defaults to whole genome\n",
    "#--snp_filter_expression \"DP>=10, GQ>=20, FS>2\" \\ # required\n",
    "#--monomorphic_filter_expression \"DP>=10, RGQ>=20\" \\ # required\n",
    "#--out pi_dxy_out.txt # default to vcf path + suffix\n",
    "\n",
    "# initialize and add arguments to the argparser\n",
    "\n",
    "parser = argparse.ArgumentParser(description='pixy: senisbly calculate pi and/or dxy from a VCF containing variant and invariant sites')\n",
    "\n",
    "parser.add_argument('--version', action='version', version='%(prog)s 1.0')\n",
    "parser.add_argument('--stats', choices=['pi', 'dxy', 'pi_dxy'], help='Which stats to to calulate from the VCF (pi, dxy, or both)', required=True)\n",
    "parser.add_argument('--vcf', type=str, nargs='?', help='Path to the input VCF', required=True)\n",
    "parser.add_argument('--zarr', type=str, nargs='?', help='Folder in which to build the Zarr database', required=True)\n",
    "parser.add_argument('--populations', type=str, nargs='?', help='Path to the populations file')\n",
    "parser.add_argument('--window_size', type=int, nargs='?', help='Window size in base pairs over which to calculate pi/dxy')\n",
    "parser.add_argument('--filter_expression', type=str, nargs='?', help='A comma separated list of filters (e.g. DP>=10,GQ>=20) to apply to SNPs', required=True)\n",
    "parser.add_argument('--invariant_filter_expression', type=str, nargs='?', help='A comma separated list of filters (e.g. DP>=10,RGQ>=20) to apply to invariant sites', required=True)\n",
    "parser.add_argument('--outfile', type=str, nargs='?', help='Path to the output file')\n",
    "\n",
    "#parser.print_help()\n",
    "\n",
    "# pull out varizbles from the parsed args\n",
    "args = parser.parse_args('--stats pi_dxy --vcf test.vcf --zarr test/path/ --populations path/to/popfile.txt --filter_expression DP>=10,GQ>=20,RGQ>=20 --invariant_filter_expression DP>=10,RGQ>=20 --outfile pixy_out.txt'.split())\n",
    "# sim_args = parser.parse_args()\n",
    "\n",
    "if (args.populations is None) and ((args.stats == 'dxy' or args.stats == 'pi_dxy')):\n",
    "    parser.error(\"--stats dxy and --stats pi_dxy requires --populations path/to/popfile.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(filter_expression='DP>=10,GQ>=20,RGQ>=20', invariant_filter_expression='DP>=10,RGQ>=20', outfile='pixy_out.txt', populations='path/to/popfile.txt', stats='pi_dxy', vcf='test.vcf', window_size=None, zarr='test/path/')\n",
      "DP>=10,GQ>=20,RGQ>=20\n"
     ]
    }
   ],
   "source": [
    "print(args)\n",
    "\n",
    "print(args.filter_expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating inputs\n",
    "\n",
    "# STEP 1 checking the vcf:\n",
    "# - check if sci-kit allele can do this\n",
    "# - check for contig info\n",
    "# - alternatively use position vector\n",
    "# - check for invariant sites and throw and error if they dont exist\n",
    "\n",
    "# STEP 2 validate the population file\n",
    "# - format is IND POP\n",
    "# - throw an error if individuals are missing from VCF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test data:\n",
    "chromosome = \"chrX\"\n",
    "vcf_path = 'data/vcf/ag1000/chrX_36Ag_allsites.vcf.gz'\n",
    "zarr_path = 'data/vcf/ag1000/chrX_36Ag_allsites.zarr'\n",
    "#vcf_path = '/Users/Katharine Korunes/Documents/Dxy_test_data/chrX_36Ag_allsites.vcf.gz'\n",
    "#zarr_path = '/Users/Katharine Korunes/Documents/Dxy_test_data/chrX_36Ag_allsites.zarr'\n",
    "# inspect the zarr data\n",
    "callset = zarr.open_group(zarr_path, mode='r')\n",
    "#callset.tree(expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the filtration expression and build the boolean filter array\n",
    "\n",
    "# define an operator dictionary for parsing the operator strings\n",
    "ops = { \"<\": operator.lt, \"<=\": operator.le, \">\": operator.gt, \">=\": operator.ge, \"==\": operator.eq}\n",
    "\n",
    "filters = []\n",
    "\n",
    "# split the filtration expressions by commas\n",
    "# parse out each component\n",
    "# apply the filter\n",
    "# TBD: handle cases where the specified filter isn't in the callset\n",
    "for x in args.filter_expression.split(\",\"):\n",
    "    stat = re.sub(\"[^A-Za-z]+\", \"\", x)\n",
    "    value = int(re.sub(\"[^0-9]+\", \"\", x))\n",
    "    compare = re.sub(\"[A-Za-z0-9]+\", \"\", x)\n",
    "    \n",
    "    if type(filters) is list:\n",
    "        filters = ops[compare](callset[chromosome + '/calldata/' + stat][:], value)\n",
    "    else:\n",
    "        if stat == 'GQ':\n",
    "            GQ_filter = ops[compare](callset[chromosome + '/calldata/' + stat][:], value)\n",
    "        elif stat == 'RGQ':\n",
    "            RGQ_filter = ops[compare](callset[chromosome + '/calldata/' + stat][:], value)\n",
    "        else :\n",
    "            filters = np.logical_and(filters, ops[compare](callset[chromosome + '/calldata/' + stat][:], value))\n",
    "\n",
    "# check if GQ and RQG exist\n",
    "# if they both exist, perform a logical OR and join them into the filter\n",
    "# otherwise, perform a logical AND to join either one into the filter\n",
    "\n",
    "try:\n",
    "    GQ_filter\n",
    "except NameError:\n",
    "    GQ_exists = False\n",
    "else:\n",
    "    GQ_exists = True\n",
    "    \n",
    "try:\n",
    "    RGQ_filter\n",
    "except NameError:\n",
    "    RGQ_exists = False\n",
    "else:\n",
    "    RGQ_exists = True\n",
    "    \n",
    "if GQ_exists & RGQ_exists:\n",
    "    filters = np.logical_and(filters, np.logical_or(GQ_filter, RGQ_filter))\n",
    "elif GQ_exists:\n",
    "    filters = np.logical_and(filters, GQ_filter)\n",
    "elif RGQ_exists:\n",
    "    filters = np.logical_and(filters, RGQ_filter)\n",
    "    \n",
    "# finally, invert the whole array \n",
    "# this is for convenience/brevity in the next section\n",
    "\n",
    "filters = np.invert(filters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the filter to the data\n",
    "# all the filters are in a boolean array ('filters') \n",
    "\n",
    "# TBD: check to see there is any data left\n",
    "# TBD: print warning for low data* think more about this\n",
    "\n",
    "# the genotype calls\n",
    "# recode the gt matrix as a Dask array (saves memory)\n",
    "gt_dask = allel.GenotypeDaskArray(callset[chromosome + '/calldata/GT'])\n",
    "\n",
    "# create a packed genotype array \n",
    "# this is a array with dims snps x samples\n",
    "# genotypes are represented by single byte codes \n",
    "# critically, as the same dims as the filters array below\n",
    "gt_array = allel.GenotypeArray(gt_dask).to_packed()\n",
    "\n",
    "# set all genotypes that fail filters to 'missing'\n",
    "# 239 = -1 (i.e. missing) for packed arrays\n",
    "gt_array[filters] = 239\n",
    "\n",
    "# remove non snps from the filtered gt array\n",
    "gt_array = np.delete(gt_array, np.where(callset[chromosome + '/variants/numalt'][:] > 1), axis=0)\n",
    "\n",
    "# convert the packed array back to a GenotypeArray\n",
    "gt_array = allel.GenotypeArray.from_packed(gt_array)\n",
    "\n",
    "# build the position array\n",
    "pos_array = allel.SortedIndex(callset[chromosome + '/variants/POS'])\n",
    "\n",
    "# remove non-snps from the position array\n",
    "pos_array = pos_array[callset[chromosome + '/variants/numalt'][:] < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic functions for comparing the genotypes at each site in a region: counts differences out of sites with data\n",
    "\n",
    "#For the given region: return average pi, # of differences, # of comparisons, and # missing.\n",
    "# this function loops over every site in a region passed to it\n",
    "def tallyRegion(gt_region):\n",
    "    total_diffs = 0\n",
    "    total_comps = 0\n",
    "    total_missing = 0\n",
    "    for site in gt_region:\n",
    "        vec = site.flatten()\n",
    "        #now we have an individual site as a numpy.ndarray, pass it to the comparison function\n",
    "        site_diffs, site_comps, missing = compareGTs(vec)\n",
    "        total_diffs += site_diffs\n",
    "        total_comps += site_comps\n",
    "        total_missing += missing\n",
    "    if total_comps > 0:\n",
    "        avg_pi = total_diffs/total_comps\n",
    "    else:\n",
    "        avg_pi = 0\n",
    "    return(avg_pi, total_diffs, total_comps, total_missing)\n",
    "\n",
    "#Out of sites with data, count differences. \n",
    "#Return the number of differences, the number of comparisons, and missing data count.\n",
    "def compareGTs(vec):\n",
    "    # use gts to select only sites with data. \n",
    "    # counting missing data as a check, but it's implicit in the length of (gts)\n",
    "    gts = []\n",
    "    missing = 0\n",
    "    for x in vec:\n",
    "        if x in (0,1):\n",
    "            gts.append(x)\n",
    "        else:\n",
    "            missing += 1\n",
    "    c = Counter(gts)\n",
    "    #print(c)\n",
    "    diffs = c[1]*c[0]\n",
    "    comps = len(list(combinations(gts, 2)))        \n",
    "    return(diffs,comps,missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculations complete and written to pixy_out.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute pi over a chosen window size\n",
    "\n",
    "# calculate pi\n",
    "\n",
    "# TBD:\n",
    "# - are any pis/dxys all zero?\n",
    "# - check if # missing == (pos2 - pos1)\n",
    "# - check if everything was either compared or missing\n",
    "# - write out summary of program parameters file* think about how this should work\n",
    "\n",
    "# total chromosome length\n",
    "# chr_length = max(pos_array)\n",
    "chr_length = 10000 # testing value\n",
    "\n",
    "# window size:\n",
    "window_size = 1000\n",
    "\n",
    "# initialize window_pos_2 \n",
    "window_pos_2 = window_size\n",
    "\n",
    "# open the output file for writing\n",
    "os.remove(args.outfile)\n",
    "outfile = open(args.outfile, 'w')\n",
    "outfile.write(\"chromosome\" + \"\\t\" + \"window_pos_1\" + \"\\t\" + \"window_pos_2\" + \"\\t\" + \"avg_pi\" + \"\\t\" + \"no_sites\" + \"\\t\" + \"total_diffs\" + \"\\t\" + \"total_comps\" + \"\\t\" + \"total_missing\" + \"\\n\")\n",
    "\n",
    "# loop over windows, compute stats and write to file\n",
    "for window_pos_1 in tqdm(range (1, chr_length, window_size)):\n",
    "    loc_region = pos_array.locate_range(window_pos_1, window_pos_2)\n",
    "    gt_region1 = gt_array[loc_region]\n",
    "    avg_pi, total_diffs, total_comps, total_missing = tallyRegion(gt_region1)\n",
    "    outfile.write(str(chromosome) + \"\\t\" + str(window_pos_1) + \"\\t\" + str(window_pos_2) + \"\\t\" + str(avg_pi) + \"\\t\" + str(len(gt_region1)) + \"\\t\" + str(total_diffs) + \"\\t\" + str(total_comps) + \"\\t\" + str(total_missing) + \"\\n\")\n",
    "    window_pos_2 += window_size\n",
    "\n",
    "# close output file and print complete message\n",
    "outfile.close()\n",
    "print(\"Calculations complete and written to \" + args.outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook pixy.ipynb to python\n",
      "[NbConvertApp] Writing 8349 bytes to pixy.py\n"
     ]
    }
   ],
   "source": [
    "# convert this notebook to a .py script\n",
    "!jupyter nbconvert --to=python pixy.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
