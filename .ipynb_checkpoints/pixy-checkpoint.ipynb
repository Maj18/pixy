{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import allel\n",
    "import zarr\n",
    "import numcodecs\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the command line arguements\n",
    "# - validate the arguements \n",
    "# - throw errors if requiremabents are missing\n",
    "# - validate the filter strings\n",
    "\n",
    "#pi_dxy --pi --dxy\\ # must include at least 1 of pi and/dxy \n",
    "#--vcf allsites.vcf.gz \\ # required\n",
    "#--zarr path/to/zarr \\  # default to the vcf folder\n",
    "#--populations popfile.txt \\ # only required for dxy\n",
    "#--window_size 10000 \\ # not required, defaults to whole genome\n",
    "#--snp_filter_expression \"DP>=10, GQ>=20, FS>2\" \\ # required\n",
    "#--monomorphic_filter_expression \"DP>=10, RGQ>=20\" \\ # required\n",
    "#--out pi_dxy_out.txt # default to vcf path + suffix\n",
    "\n",
    "# initialize and add arguments to the argparser\n",
    "\n",
    "parser = argparse.ArgumentParser(description='pixy: senisbly calculate pi and/or dxy from a VCF containing variant and invariant sites')\n",
    "\n",
    "parser.add_argument('--version', action='version', version='%(prog)s 1.0')\n",
    "parser.add_argument('--stats', choices=['pi', 'dxy', 'pi_dxy'], help='Which stats to to calulate from the VCF (pi, dxy, or both)', required=True)\n",
    "parser.add_argument('--vcf', type=str, nargs='?', help='Path to the input VCF', required=True)\n",
    "parser.add_argument('--zarr', type=str, nargs='?', help='Folder in which to build the Zarr array', required=True)\n",
    "parser.add_argument('--regenerate_zarr', choices=['yes', 'no'], help='Force regeneration of the Zarr array')\n",
    "parser.add_argument('--populations', type=str, nargs='?', help='Path to the populations file')\n",
    "parser.add_argument('--window_size', type=int, nargs='?', help='Window size in base pairs over which to calculate pi/dxy')\n",
    "parser.add_argument('--filter_expression', type=str, nargs='?', help='A comma separated list of filters (e.g. DP>=10,GQ>=20) to apply to SNPs', required=True)\n",
    "parser.add_argument('--invariant_filter_expression', type=str, nargs='?', help='A comma separated list of filters (e.g. DP>=10,RGQ>=20) to apply to invariant sites', required=True)\n",
    "parser.add_argument('--outfile_prefix', type=str, nargs='?', help='Path and prefix for the output file, e.g. path/to/outfile')\n",
    "\n",
    "#parser.print_help()\n",
    "\n",
    "# pull out varizbles from the parsed args\n",
    "args = parser.parse_args('--stats pi_dxy --vcf test.vcf --zarr test/path/ --populations data/vcf/ag1000/Ag1000_sampleIDs_popfile.txt --regenerate_zarr no --filter_expression DP>=10,GQ>=20,RGQ>=20 --invariant_filter_expression DP>=10,RGQ>=20 --outfile_prefix output/pixy_out'.split())\n",
    "# sim_args = parser.parse_args()\n",
    "\n",
    "if (args.populations is None) and ((args.stats == 'dxy' or args.stats == 'pi_dxy')):\n",
    "    parser.error(\"--stats dxy and --stats pi_dxy requires --populations path/to/popfile.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(filter_expression='DP>=10,GQ>=20,RGQ>=20', invariant_filter_expression='DP>=10,RGQ>=20', outfile_prefix='output/pixy_out', populations='data/vcf/ag1000/Ag1000_sampleIDs_popfile.txt', regenerate_zarr='no', stats='pi_dxy', vcf='test.vcf', window_size=None, zarr='test/path/')\n",
      "DP>=10,GQ>=20,RGQ>=20\n"
     ]
    }
   ],
   "source": [
    "print(args)\n",
    "\n",
    "print(args.filter_expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating inputs\n",
    "\n",
    "# STEP 1 checking the vcf:\n",
    "# - check if sci-kit allele can do this\n",
    "# - check for contig info\n",
    "# - alternatively use position vector\n",
    "# - check for invariant sites and throw and error if they dont exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zarr array conversion\n",
    "\n",
    "# test data\n",
    "# TBD: replace with parsed args\n",
    "chromosome = \"chrX\"\n",
    "vcf_path = 'data/vcf/ag1000/chrX_36Ag_allsites.vcf.gz'\n",
    "zarr_path = 'data/vcf/ag1000/chrX_36Ag_allsites.zarr'\n",
    "\n",
    "#vcf_path = '/Users/Katharine Korunes/Documents/Dxy_test_data/chrX_36Ag_allsites.vcf.gz'\n",
    "#zarr_path = '/Users/Katharine Korunes/Documents/Dxy_test_data/chrX_36Ag_allsites.zarr'\n",
    "\n",
    "# perform the vcf to zarr conversion if the zarr array is missing, or regeneration has been requested\n",
    "\n",
    "if os.path.exists(zarr_path) is not True:\n",
    "    print(\"Zarr array does not exist, building...\")\n",
    "    allel.vcf_to_zarr(vcf_path, zarr_path, group='chrX', fields='*', log=sys.stdout, overwrite=True)\n",
    "elif 'regenerate_zarr' in args:\n",
    "    if args.regenerate_zarr == 'yes':\n",
    "        print(\"Regenerating Zarr array...\")\n",
    "        allel.vcf_to_zarr(vcf_path, zarr_path, group='chrX', fields='*', log=sys.stdout, overwrite=True)\n",
    "\n",
    "# inspect the structure of the zarr data\n",
    "callset = zarr.open_group(zarr_path, mode='r')\n",
    "\n",
    "#callset.tree(expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VCF samples: ['ERS223750', 'ERS223759', 'ERS223771', 'ERS223790', 'ERS223797', 'ERS223827', 'ERS223924', 'ERS223967', 'ERS223970', 'ERS224089', 'ERS224091', 'ERS224095', 'ERS224105', 'ERS224112', 'ERS224115', 'ERS224153', 'ERS224165', 'ERS224167', 'ERS224168', 'ERS224170', 'ERS224205', 'ERS224206', 'ERS224209', 'ERS224217', 'ERS224218', 'ERS224224', 'ERS224227', 'ERS224235', 'ERS224245', 'ERS224248', 'ERS224283', 'ERS224294', 'ERS224295', 'ERS224300', 'ERS224314', 'ERS224670']\n",
      "BFS\n",
      "KES\n",
      "{'BFS': array([35, 29,  9, 30, 31, 16, 11, 32, 26,  3,  4,  2,  5,  1,  0,  7,  8,\n",
      "        6]), 'KES': array([33, 18, 34, 27, 28, 21, 15, 10, 12, 13, 17, 22, 25, 23, 24, 14, 20,\n",
      "       19])}\n"
     ]
    }
   ],
   "source": [
    "# STEP 2 prase + validate the population file\n",
    "# - format is IND POP (tab separated)\n",
    "# - throws an error if individuals are missing from VCF\n",
    "\n",
    "# read in the list of samples/populations\n",
    "poppanel = pandas.read_csv(args.populations, sep='\\t', usecols=[0,1], names=['ID', 'Population'])\n",
    "poppanel.head()\n",
    "\n",
    "# get a list of samples from the callset\n",
    "samples = callset[chromosome + '/samples'][:]\n",
    "samples_list = list(samples)\n",
    "print('VCF samples:', samples_list)\n",
    "\n",
    "#make sure every indiv in the pop file is in the VCF callset\n",
    "IDs = list(poppanel['ID'])\n",
    "missing = list(set(IDs)-set(samples_list))\n",
    "if len(missing) > 0:\n",
    "    print('ERROR: there are samples in the population file that are not in the VCF -', missing)\n",
    "    \n",
    "# find the samples in the callset index by matching up the order of samples between the population file and the callset\n",
    "samples_callset_index = [samples_list.index(s) for s in poppanel['ID']]\n",
    "poppanel['callset_index'] = samples_callset_index\n",
    "\n",
    "# use the popindices dictionary to keep track of the indices for each population\n",
    "popindices={}\n",
    "popnames = poppanel.Population.unique()\n",
    "for name in popnames:\n",
    "    print(name)\n",
    "    popindices[name] = poppanel[poppanel.Population == name].callset_index.values\n",
    "\n",
    "print(popindices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating filters...\n",
      "DP >= 10\n",
      "GQ >= 20\n",
      "RGQ >= 20\n"
     ]
    }
   ],
   "source": [
    "# parse the filtration expression and build the boolean filter array\n",
    "\n",
    "# define an operator dictionary for parsing the operator strings\n",
    "ops = { \"<\": operator.lt, \"<=\": operator.le, \">\": operator.gt, \">=\": operator.ge, \"==\": operator.eq}\n",
    "\n",
    "filters = []\n",
    "\n",
    "print(\"Creating filters...\")\n",
    "\n",
    "# split the filtration expressions by commas\n",
    "# parse out each component\n",
    "# apply the filter\n",
    "# TBD: handle cases where the specified filter isn't in the callset\n",
    "for x in args.filter_expression.split(\",\"):\n",
    "    stat = re.sub(\"[^A-Za-z]+\", \"\", x)\n",
    "    value = int(re.sub(\"[^0-9]+\", \"\", x))\n",
    "    compare = re.sub(\"[A-Za-z0-9]+\", \"\", x)\n",
    "    \n",
    "    print(str(stat) + \" \" + str(compare) + \" \" + str(value))\n",
    "    \n",
    "    if type(filters) is list:\n",
    "        filters = ops[compare](callset[chromosome + '/calldata/' + stat][:], value)\n",
    "    else:\n",
    "        if stat == 'GQ':\n",
    "            GQ_filter = ops[compare](callset[chromosome + '/calldata/' + stat][:], value)\n",
    "        elif stat == 'RGQ':\n",
    "            RGQ_filter = ops[compare](callset[chromosome + '/calldata/' + stat][:], value)\n",
    "        else :\n",
    "            filters = np.logical_and(filters, ops[compare](callset[chromosome + '/calldata/' + stat][:], value))\n",
    "\n",
    "# check if GQ and RQG exist\n",
    "# if they both exist, perform a logical OR and join them into the filter\n",
    "# otherwise, perform a logical AND to join either one into the filter\n",
    "\n",
    "GQ_exists = 'GQ_filter' in args\n",
    "RGQ_exists = 'RGQ_filter' in args\n",
    "   \n",
    "if GQ_exists & RGQ_exists:\n",
    "    filters = np.logical_and(filters, np.logical_or(GQ_filter, RGQ_filter))\n",
    "elif GQ_exists:\n",
    "    filters = np.logical_and(filters, GQ_filter)\n",
    "elif RGQ_exists:\n",
    "    filters = np.logical_and(filters, RGQ_filter)\n",
    "        \n",
    "# finally, invert the whole array \n",
    "# this is for convenience/brevity in the next section\n",
    "\n",
    "filters = np.invert(filters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying filters...\n"
     ]
    }
   ],
   "source": [
    "# applying the filter to the data\n",
    "# all the filters are in a boolean array ('filters') \n",
    "\n",
    "# TBD: check to see there is any data left\n",
    "# TBD: print warning for low data* think more about this\n",
    "\n",
    "print(\"Applying filters...\")\n",
    "\n",
    "# the genotype calls\n",
    "# recode the gt matrix as a Dask array (saves memory)\n",
    "gt_dask = allel.GenotypeDaskArray(callset[chromosome + '/calldata/GT'])\n",
    "\n",
    "# create a packed genotype array \n",
    "# this is a array with dims snps x samples\n",
    "# genotypes are represented by single byte codes \n",
    "# critically, as the same dims as the filters array below\n",
    "gt_array = allel.GenotypeArray(gt_dask).to_packed()\n",
    "\n",
    "# set all genotypes that fail filters to 'missing'\n",
    "# 239 = -1 (i.e. missing) for packed arrays\n",
    "gt_array[filters] = 239\n",
    "\n",
    "# remove sites with >1 alt allele from the filtered gt array\n",
    "gt_array = np.delete(gt_array, np.where(callset[chromosome + '/variants/numalt'][:] > 1), axis=0)\n",
    "\n",
    "# convert the packed array back to a GenotypeArray\n",
    "gt_array = allel.GenotypeArray.from_packed(gt_array)\n",
    "\n",
    "# build the position array\n",
    "pos_array = allel.SortedIndex(callset[chromosome + '/variants/POS'])\n",
    "\n",
    "# remove non-snps from the position array\n",
    "pos_array = pos_array[callset[chromosome + '/variants/numalt'][:] < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic functions for comparing the genotypes at each site in a region: counts differences out of sites with data\n",
    "\n",
    "#For the given region: return average pi, # of differences, # of comparisons, and # missing.\n",
    "# this function loops over every site in a region passed to it\n",
    "\n",
    "#Basic functions for comparing the genotypes at each site in a region: counts differences out of sites with data\n",
    "\n",
    "#For the given region: return average pi, # of differences, # of comparisons, and # missing.\n",
    "# this function loops over every site in a region passed to it\n",
    "def tallyRegion(gt_region):\n",
    "    total_diffs = 0\n",
    "    total_comps = 0\n",
    "    total_missing = 0\n",
    "    for site in gt_region:\n",
    "        vec = site.flatten()\n",
    "        #now we have an individual site as a numpy.ndarray, pass it to the comparison function\n",
    "        site_diffs, site_comps, missing = compareGTs(vec)\n",
    "        total_diffs += site_diffs\n",
    "        total_comps += site_comps\n",
    "        total_missing += missing\n",
    "    if total_comps > 0:\n",
    "        avg_pi = total_diffs/total_comps\n",
    "    else:\n",
    "        avg_pi = 0\n",
    "    return(avg_pi, total_diffs, total_comps, total_missing)\n",
    "\n",
    "#For the given region: return average dxy, # of differences, # of comparisons, and # missing.\n",
    "# this function loops over every site in a region passed to it\n",
    "def dxyTallyRegion(pop1_gt_region, pop2_gt_region):\n",
    "    total_diffs = 0\n",
    "    total_comps = 0\n",
    "    total_missing = 0\n",
    "    for x in range(0,len(pop1_gt_region)):\n",
    "        site1 = pop1_gt_region[x]\n",
    "        site2 = pop2_gt_region[x]\n",
    "        vec1 = site1.flatten()\n",
    "        vec2 = site2.flatten()\n",
    "        #now we have an individual site as 2 numpy.ndarrays, pass them to the comparison function\n",
    "        site_diffs, site_comps, missing = dxyCompareGTs(vec1, vec2)\n",
    "        total_diffs += site_diffs\n",
    "        total_comps += site_comps\n",
    "        total_missing += missing\n",
    "    if total_comps > 0:\n",
    "        avg_pi = total_diffs/total_comps\n",
    "    else:\n",
    "        avg_pi = 0\n",
    "    return(avg_pi, total_diffs, total_comps, total_missing)\n",
    "\n",
    "#Out of sites with data, count differences. \n",
    "#Return the number of differences, the number of comparisons, and missing data count.\n",
    "def compareGTs(vec):\n",
    "    # use gts to select only sites with data. \n",
    "    # counting missing data as a check, but it's implicit in the length of (gts)\n",
    "    gts = []\n",
    "    missing = 0\n",
    "    for x in vec:\n",
    "        if x in (0,1):\n",
    "            gts.append(x)\n",
    "        else:\n",
    "            missing += 1\n",
    "    c = Counter(gts)\n",
    "    #print(c)\n",
    "    diffs = c[1]*c[0]\n",
    "    comps = len(list(combinations(gts, 2)))        \n",
    "    return(diffs,comps,missing)\n",
    "\n",
    "def dxyCompareGTs(vec1, vec2):\n",
    "    # use gts to select only sites with data. counting missing data as a check, but it's implicit in the length of (gts)\n",
    "    #gts for each of the 2 populations being compared:\n",
    "    gts1 = []\n",
    "    gts2 = []\n",
    "    missing = 0\n",
    "    for x in vec1:\n",
    "        if x in (0,1):\n",
    "            gts1.append(x)\n",
    "        else:\n",
    "            missing += 1\n",
    "    for y in vec2:\n",
    "        if y in (0,1):\n",
    "            gts2.append(y)\n",
    "        else:\n",
    "            missing += 1\n",
    "     \n",
    "    diffs = 0\n",
    "    comps = 0\n",
    "    length1 = len(gts1)\n",
    "    length2 = len(gts2)\n",
    "    for x in range(0,length1):\n",
    "        i = gts1[x]\n",
    "        for n in range(0,length2):\n",
    "            j = gts2[n]\n",
    "            comps += 1\n",
    "            if i != j:\n",
    "                diffs += 1    \n",
    "        \n",
    "    return(diffs,comps,missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  8.43it/s]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi calculations complete and written to output/pixy_out_[popname]_pi.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PI:\n",
    "# AVERAGE NUCLEOTIDE VARIATION WITHIN POPULATIONS\n",
    "\n",
    "# Compute pi over a chosen window size\n",
    "\n",
    "# calculate pi\n",
    "\n",
    "# TBD:\n",
    "# - are any pis/dxys all zero?\n",
    "# - check if # missing == (pos2 - pos1)\n",
    "# - check if everything was either compared or missing\n",
    "# - write out summary of program parameters file* think about how this should work\n",
    "# - if there are >1 populations, do this for each separately!\n",
    "\n",
    "# total chromosome length\n",
    "# chr_length = max(pos_array)\n",
    "\n",
    "if (args.populations is not None) and ((args.stats == 'pi' or args.stats == 'pi_dxy')):\n",
    "\n",
    "    chr_length = 10000 # testing value\n",
    "\n",
    "    # window size:\n",
    "    window_size = 1000\n",
    "\n",
    "    # initialize window_pos_2 \n",
    "    window_pos_2 = window_size\n",
    "\n",
    "    # initialize the pi output file names\n",
    "\n",
    "    for pop in popnames:\n",
    "\n",
    "        # create pi name via the prefix\n",
    "        pi_file = str(args.outfile_prefix) + \"_\" + str(pop) +\"_pi.txt\"\n",
    "\n",
    "        # remove any existing pop files\n",
    "        if os.path.exists(pi_file):\n",
    "            os.remove(pi_file)\n",
    "\n",
    "        # open the dxy output file for writing\n",
    "        outfile = open(pi_file, 'w')\n",
    "        outfile.write(\"pop\" + \"\\t\" + \"chromosome\" + \"\\t\" + \"window_pos_1\" + \"\\t\" + \"window_pos_2\" + \"\\t\" + \"avg_pi\" + \"\\t\" + \"no_sites\" + \"\\t\" + \"count_diffs\" + \"\\t\" + \"count_comparisons\" + \"\\t\" + \"count_missing\" + \"\\n\")\n",
    "\n",
    "        # loop over populations and windows, compute stats and write to file\n",
    "        for window_pos_1 in tqdm(range (1, chr_length, window_size)):\n",
    "\n",
    "            # pull out the genotypes for the window\n",
    "            loc_region = pos_array.locate_range(window_pos_1, window_pos_2)\n",
    "            gt_region1 = gt_array[loc_region]\n",
    "\n",
    "            # subset the window for the individuals in each population \n",
    "            gt_pop = gt_region1.take(popindices[pop], axis=1)\n",
    "\n",
    "            avg_pi, total_diffs, total_comps, total_missing = tallyRegion(gt_pop)\n",
    "            outfile.write(str(pop) + \"\\t\" + str(chromosome) + \"\\t\" + str(window_pos_1) + \"\\t\" + str(window_pos_2) + \"\\t\" + str(avg_pi) + \"\\t\" + str(len(gt_region1)) + \"\\t\" + str(total_diffs) + \"\\t\" + str(total_comps) + \"\\t\" + str(total_missing) + \"\\n\")\n",
    "            window_pos_2 += window_size\n",
    "\n",
    "        # close output file and print complete message\n",
    "        outfile.close()\n",
    "\n",
    "    print(\"Pi calculations complete and written to \" + args.outfile_prefix + \"_[popname]_pi.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dxy calculations complete and written to output/pixy_out_[pop1]_[pop2]_dxy.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# DXY:\n",
    "# AVERAGE NUCLEOTIDE VARIATION BETWEEN POPULATIONS\n",
    "\n",
    "#come back to this later (parsing arguments to dictate pi/dxy behaviour)\n",
    "#if (args.populations is not None) and ((args.stats == 'dxy' or args.stats == 'pi_dxy'))\n",
    "\n",
    "# total chromosome length\n",
    "# chr_length = max(pos_array)\n",
    "\n",
    "if (args.populations is not None) and ((args.stats == 'dxy' or args.stats == 'pi_dxy')):\n",
    "\n",
    "    chr_length = 10000 # testing value, total length of the chromosome\n",
    "\n",
    "    # window size:\n",
    "    window_size = 1000\n",
    "\n",
    "    # initialize window_pos_2 \n",
    "    window_pos_2 = window_size\n",
    "\n",
    "    # create a list of all pairwise comparisons between populations in the popfile\n",
    "    dxy_pop_list = list(itertools.combinations(popnames, 2))\n",
    "\n",
    "    # interate over all population pairs and compute dxy\n",
    "    for pop_pair in dxy_pop_list:\n",
    "        pop1 = pop_pair[0]\n",
    "        pop2 = pop_pair[1]\n",
    "\n",
    "        # rename the dxy output file based on the prefix\n",
    "        dxy_file = str(args.outfile_prefix) + \"_\" + str(pop1) + \"_\" + str(pop2) +\"_dxy.txt\"\n",
    "\n",
    "        # remove any previous results\n",
    "        if os.path.exists(dxy_file):\n",
    "            os.remove(dxy_file)\n",
    "\n",
    "        # open the dxy output file for writing\n",
    "        outfile = open(dxy_file, 'w')\n",
    "        outfile.write(\"pop1\" + \"\\t\" + \"pop2\" + \"\\t\" + \"chromosome\" + \"\\t\" + \"window_pos_1\" + \"\\t\" + \"window_pos_2\" + \"\\t\" + \"avg_dxy\" + \"\\t\" + \"no_sites\" + \"\\t\" + \"count_diffs\" + \"\\t\" + \"count_comparisons\" + \"\\t\" + \"count_missing\" + \"\\n\")\n",
    "\n",
    "        # perform the dxy calculation for all windows in the range\n",
    "        for window_pos_1 in tqdm(range (1, chr_length, window_size)):\n",
    "            loc_region = pos_array.locate_range(window_pos_1, window_pos_2)\n",
    "            gt_region1 = gt_array[loc_region]\n",
    "            # use the popGTs dictionary to keep track of this region's GTs for each population\n",
    "            popGTs={}\n",
    "            for name in pop_pair:\n",
    "                #print(popindices[name])\n",
    "                gt_pop = gt_region1.take(popindices[name], axis=1)\n",
    "                popGTs[name] = gt_pop\n",
    "            #print(popGTs)\n",
    "            pop1_gt_region1 = popGTs[pop1]\n",
    "            pop2_gt_region1 = popGTs[pop2]\n",
    "            avg_dxy, total_diffs, total_comps, total_missing = dxyTallyRegion(pop1_gt_region1, pop2_gt_region1)\n",
    "            outfile.write(str(pop1) + \"\\t\" + str(pop2) + \"\\t\" + str(chromosome) + \"\\t\" + str(window_pos_1) + \"\\t\" + str(window_pos_2) + \"\\t\" + str(avg_dxy) + \"\\t\" + str(len(gt_region1)) + \"\\t\" + str(total_diffs) + \"\\t\" + str(total_comps) + \"\\t\" + str(total_missing) + \"\\n\")\n",
    "\n",
    "            #print(\"Region:\", x , y, \",\", \"Region length:\", len(pop1_gt_region1))\n",
    "            #print(\"Average dxy:\", avg_dxy)\n",
    "            #print(\"Diffs, comps, missing:\", total_diffs, total_comps, total_missing, \"\\n\")\n",
    "            window_pos_2 += window_size\n",
    "\n",
    "        outfile.close()\n",
    "\n",
    "    print(\"Dxy calculations complete and written to \" + args.outfile_prefix + \"_[pop1]_[pop2]_dxy.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert this notebook to a .py script\n",
    "!jupyter nbconvert --to=python pixy.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
